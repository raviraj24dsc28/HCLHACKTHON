# -*- coding: utf-8 -*-
"""Untitled3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1F96tYIVAz8pMz1hImGbySjmfwk7P7XbE
"""

# =========================================================
# 30-Day Customer Spend Prediction (CLV) - FULL ONE CELL CODE
# =========================================================

# ---------- STEP 1: IMPORT LIBRARIES ----------
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVR
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import joblib


# ---------- STEP 2: LOAD DATASET ----------
# Make sure CSV is in same folder as notebook
df = pd.read_csv("Retail_Transaction_Dataset.csv", encoding="latin1")

print("Dataset Shape:", df.shape)
print(df.head())


# ---------- STEP 3: BASIC DATA CLEANING ----------
df.dropna(inplace=True)

# Convert date column
df["TransactionDate"] = pd.to_datetime(df["TransactionDate"])

# Create TotalAmount if not present
if "TotalAmount" not in df.columns:
    df["TotalAmount"] = df["Quantity"] * df["Price"]


# ---------- STEP 4: DEFINE CUTOFF DATE (30 DAYS) ----------
cutoff_date = df["TransactionDate"].max() - pd.Timedelta(days=30)

past_data = df[df["TransactionDate"] <= cutoff_date]
future_data = df[df["TransactionDate"] > cutoff_date]


# ---------- STEP 5: FEATURE ENGINEERING (INPUT VARIABLES) ----------
customer_features = past_data.groupby("CustomerID").agg(
    recency=("TransactionDate", lambda x: (cutoff_date - x.max()).days),
    frequency=("TransactionDate", "count"),
    monetary=("TotalAmount", "sum"),
    avg_discount=("DiscountApplied(%)", "mean"),
    total_quantity=("Quantity", "sum")
).reset_index()


# ---------- STEP 6: TARGET VARIABLE (OUTPUT VARIABLE) ----------
clv_30 = future_data.groupby("CustomerID")["TotalAmount"].sum().reset_index()
clv_30.columns = ["CustomerID", "clv_30"]

final_data = customer_features.merge(clv_30, on="CustomerID", how="left")
final_data["clv_30"] = final_data["clv_30"].fillna(0)

print("\nFinal Modeling Data:")
print(final_data.head())


# ---------- STEP 7: DEFINE X (INPUT) AND y (OUTPUT) ----------
X = final_data.drop(["CustomerID", "clv_30"], axis=1)
y = final_data["clv_30"]

# Reset index to avoid leakage
X = X.reset_index(drop=True)
y = y.reset_index(drop=True)


# ---------- STEP 8: TRAINâ€“TEST SPLIT ----------
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)


# ---------- STEP 9: BUILD MODEL (SCALING + SVR) ----------
svr_pipeline = Pipeline([
    ("scaler", StandardScaler()),
    ("svr", SVR(kernel="rbf", C=10, epsilon=0.1))
])


# ---------- STEP 10: TRAIN MODEL ----------
svr_pipeline.fit(X_train, y_train)


# ---------- STEP 11: PREDICTION ----------
y_pred = svr_pipeline.predict(X_test)


# ---------- STEP 12: MODEL EVALUATION ----------
print("\nMODEL PERFORMANCE (SVR)")

print("MAE :", mean_absolute_error(y_test, y_pred))
print("RMSE:", np.sqrt(mean_squared_error(y_test, y_pred)))
print("R2  :", r2_score(y_test, y_pred))

# Sanity check (must be False)
print("Prediction equals actual?",
      np.allclose(y_test.values, y_pred))


# ---------- STEP 13: SAVE FINAL MODEL ----------
joblib.dump(svr_pipeline, "final_svr_clv_model.pkl")

print("\nModel saved as: final_svr_clv_model.pkl")